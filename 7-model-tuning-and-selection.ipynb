{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "![Model Tuning and Selection banner](./images/7_model_tuning_and_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "# 7. Model Tuning and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0072d-fa79-451e-ad9f-3c61cfb28a8a",
   "metadata": {},
   "source": [
    "## 7.1. Hyperparameter Tuning\n",
    "\n",
    "The next step after selecting a model and training it is to tune the hyperparameters to optimize performance. Hyperparameters are settings that control the learning process of the model, such as learning rate, regularization strength, number of trees, etc.\n",
    "\n",
    "There are several techniques for hyperparameter tuning:\n",
    "- **Grid search**: A brute-force approach that exhaustively builds and evaluates models for every combination of hyperparameter values within a pre-specified search space. Computationally expensive but will not miss any points.\n",
    "- **Random search**: Instead of a full grid, random search evaluates models with randomly sampled hyperparameter values from the search space. More efficient than grid search for higher dimensional problems.\n",
    "- **Bayesian optimization**: An intelligent search that uses a probabilistic model to guide the hyperparameter selection process. It aims to find the optimal values with fewer iterations by leveraging information from previous evaluations.\n",
    "\n",
    "The goal is to find the hyperparameter values that lead to the best performance on a held-out validation set. This is often done within a cross-validation loop to ensure the selected hyperparameters generalize well.\n",
    "\n",
    "![Model Tuning and Selection loops](./images/7_model_tuning_and_selection_loops.png)\n",
    "\n",
    "Some other important considerations:\n",
    "- Feature engineering can have a big impact on model performance\n",
    "- Evaluate using appropriate metrics for the problem (accuracy, F1, RMSE, etc.)\n",
    "- Monitor for overfitting and underfitting during tuning\n",
    "- Techniques like early stopping can help prevent overfitting\n",
    "\n",
    "So in summary, hyperparameter tuning is a critical step to optimize a model's performance after selecting and training it. Techniques like grid search, random search, and Bayesian optimization are commonly used to find the best hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f043b-09e2-4e5c-959e-baaeaacc0e7d",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 7.2. Ensemble Methods\n",
    "\n",
    "Ensemble methods are techniques that create multiple models and combine them to produce improved results compared to a single model. The main idea is that when weak models are correctly combined, more accurate and robust models can be obtained.\n",
    "\n",
    "### 7.2.1. Bagging\n",
    "\n",
    "Bagging involves training multiple base models independently on different random subsets of the training data, then combining their predictions through techniques like voting for classification or averaging for regression. Popular bagging methods include:\n",
    "- **Random Forests**: An ensemble of decision trees, each trained on a bootstrap sample of the data using random subsets of features.\n",
    "- **Bootstrap Aggregating (Bagging)**: Fits separate models on random subsets of data and averages their predictions.\n",
    "\n",
    "Bagging aims to reduce the variance of individual models, making the ensemble more robust to overfitting.\n",
    "\n",
    "### 7.2.2. Boosting\n",
    "\n",
    "Boosting trains models sequentially, with each new model focusing on the instances misclassified by the previous one. The final prediction is a weighted combination of the weak learners. Key boosting algorithms include:\n",
    "- **AdaBoost**: Iteratively adjusts weights of training instances based on their difficulty.\n",
    "- **Gradient Boosting**: Builds additive models by sequentially training weak learners on the negative gradients of the loss function.\n",
    "\n",
    "Boosting reduces bias and variance, allowing weak learners to become strong predictors.\n",
    "\n",
    "### 7.2.3. Stacking\n",
    "\n",
    "Stacking involves training different types of models on the same data, then using a meta-model to combine their predictions optimally. This allows leveraging strengths of diverse base models.\n",
    "\n",
    "The success of ensemble methods depends on factors like diversity among base models, data sampling strategies, and the technique used to combine predictions. Ensembles have consistently achieved top results in machine learning competitions by reducing overfitting and capturing different aspects of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1dcd3-0d96-440b-ac31-47d8236756f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
