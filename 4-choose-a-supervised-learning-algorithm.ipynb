{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "![Choose a Supervised Learning Algorithm banner](./images/4_choose_a_supervised_learning_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "# 4. Choose an algorithm\n",
    "\n",
    "Select an appropriate supervised learning algorithm based on the problem type (classification or regression), data characteristics, interpretability requirements, training time, and other practical considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0dcf1-b0e1-4bf1-b0e8-15d0eb60c60e",
   "metadata": {},
   "source": [
    "## 4.1. Consider algorithm categories\n",
    "\n",
    "![Choose supervised learning algorithm cheat sheet](./images/4_choose_a_supervised_learning_algorithm_scikit_learn_cheat_sheet.png)\n",
    "\n",
    "Machine learning algorithms can be broadly categorized into three main types:\n",
    "\n",
    "**Supervised Learning**: These algorithms learn from labeled data, where the input data has corresponding output labels or target variables. The goal is to learn a mapping function from the input features to the output labels. Common supervised learning tasks include classification (predicting a categorical label) and regression (predicting a continuous value).\n",
    "\n",
    "- **Some algorithms**: Linear Regression, Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVMs), Neural Networks.\n",
    "\n",
    "Examples by dataset:\n",
    "  - House Prices (Regression) - Linear Regression for interpretability, XGBoost for competition performance\n",
    "  - Email Spam (Classification) - Naive Bayes excels with text data, achieving 98% accuracy on Enron dataset\n",
    "  - MNIST Digits - Neural Networks (CNNs) achieve 99.7% accuracy, SVMs reach 98.5%\n",
    "\n",
    "**Unsupervised Learning**: These algorithms learn from unlabeled data, where there are no predefined output labels. The goal is to discover patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features), and association rule mining.\n",
    "\n",
    "- **Some algorithms**: K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), Association Rule Mining.\n",
    "\n",
    "Examples by dataset:\n",
    "  - Customer Segmentation - K-Means on RFM (Recency, Frequency, Monetary) features to identify customer groups\n",
    "  - Gene Expression Data - PCA reduces thousands of genes to principal components for visualization\n",
    "  - Market Basket Analysis - Association rules find \"beer and diapers\" patterns in retail transaction data\n",
    "\n",
    "**Semi-Supervised Learning**: These algorithms combine a small amount of labeled data with a large amount of unlabeled data. They leverage the strengths of both supervised and unsupervised learning techniques to improve model performance, especially when labeled data is scarce or expensive to obtain.\n",
    "\n",
    "- **Some algorithms**: Self-Training, Co-Training, Generative Adversarial Networks (GANs).\n",
    "\n",
    "Examples by dataset:\n",
    "  - Medical Imaging - Only 100 labeled X-rays but 10,000 unlabeled; semi-supervised learning improves accuracy\n",
    "  - Text Classification - Using 1,000 labeled documents + 100,000 unlabeled web pages for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad863d4-3862-4f52-b611-69c5d0bf3ed9",
   "metadata": {},
   "source": [
    "## 4.2. Evaluate algorithm characteristics\n",
    "\n",
    "When evaluating different supervised learning algorithms, consider the following characteristics:\n",
    "\n",
    "- **Interpretability**: Some algorithms, like linear models and decision trees, are more interpretable and provide insights into the relationship between input features and the target variable. Others, like neural networks, are more complex and can be treated as \"black boxes.\"\n",
    "\n",
    "  Examples:\n",
    "    - Medical Diagnosis - Doctors require Decision Trees to explain why a patient is high-risk\n",
    "    - Credit Scoring - Regulations require Linear Regression coefficients to justify loan denials\n",
    "    - Image Recognition - Deep learning black box acceptable since accuracy matters more than explanation\n",
    "\n",
    "- **Training Time**: Some algorithms, like linear models, are computationally efficient and can be trained quickly, even on large datasets. Others, like ensemble methods (e.g., Random Forests) and neural networks, may require more computational resources and longer training times.\n",
    "\n",
    "  Examples:\n",
    "    - Real-time Fraud Detection - Logistic Regression trains in seconds on millions of transactions\n",
    "    - Kaggle Competitions - XGBoost may take hours but worth it for 1% accuracy gain\n",
    "    - GPT Models - Months of training on thousands of GPUs for state-of-the-art NLP\n",
    "\n",
    "- **Prediction Speed**: After training, some algorithms can make predictions very quickly (e.g., linear models), while others may be slower (e.g., instance-based methods like k-Nearest Neighbors).\n",
    "\n",
    "  Examples:\n",
    "    - High-Frequency Trading - Linear models make predictions in microseconds\n",
    "    - Recommendation Systems - kNN too slow for real-time; use matrix factorization instead\n",
    "    - Mobile Apps - Decision trees work offline; neural networks need cloud inference\n",
    "\n",
    "- **Data Type Handling**: Some algorithms can handle different data types (e.g., categorical, numerical, text) natively, while others may require additional data preprocessing or feature engineering.\n",
    "\n",
    "  Examples:\n",
    "    - Mixed Data (Titanic) - Random Forests handle categorical/numerical without encoding\n",
    "    - Text Classification - Naive Bayes naturally works with word counts\n",
    "    - Tabular Data - XGBoost handles missing values without imputation\n",
    "\n",
    "- **Robustness to Outliers**: Some algorithms, like decision trees and ensemble methods, are more robust to outliers in the data, while others, like linear models, can be heavily influenced by outliers.\n",
    "\n",
    "  Examples:\n",
    "    - Income Prediction - Tree-based models handle millionaire outliers better than Linear Regression\n",
    "    - Sensor Data - Random Forest robust to occasional sensor malfunctions\n",
    "    - Student Grades - One bad test score skews Linear Regression predictions\n",
    "\n",
    "- **Scalability**: As the size of the dataset grows, some algorithms may become computationally expensive or require specialized techniques (e.g., online learning, distributed computing) to handle large-scale data.\n",
    " \n",
    "  Examples:\n",
    "    - Google Search - SGD (Stochastic Gradient Descent) scales to billions of examples\n",
    "    - IoT Sensors - Online learning updates model with each new data point\n",
    "\n",
    "Evaluating these characteristics can help narrow down the choices and select algorithms that are well-suited for your specific problem and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d767ab5-b6b8-4a61-95c1-0a193d61f736",
   "metadata": {},
   "source": [
    "## 4.3. Try multiple algorithms\n",
    "\n",
    "Since it's difficult to know the best algorithm upfront, it's recommended to try multiple algorithms from different families (e.g., linear models, tree-based models, instance-based models, etc.) and compare their performance on specific data. \n",
    "\n",
    "This process is often referred to as \"model selection\" or \"algorithm selection\".\n",
    "\n",
    "Here are some common algorithm families and examples:\n",
    "\n",
    "- **Linear Models**: Linear Regression, Logistic Regression, Support Vector Machines (SVMs).\n",
    "\n",
    "  Best for:\n",
    "    - Boston Housing - Linear Regression achieves R² of 0.74 with clear feature importance\n",
    "    - Iris Classification - Logistic Regression gets 97% accuracy on linearly separable species\n",
    "    - Text Classification - Linear SVM excels on high-dimensional sparse text features\n",
    "\n",
    "- **Tree-Based Models**: Decision Trees, Random Forests, Gradient Boosting Machines.\n",
    "\n",
    "  Best for:\n",
    "    - Titanic Survival - Random Forest captures complex interactions (gender × class × age)\n",
    "    - House Prices Advanced - XGBoost wins most Kaggle competitions with 0.12 RMSE\n",
    "    - Credit Default - Decision Trees provide clear if-then rules for loan officers\n",
    "\n",
    "- **Instance-Based Models**: k-Nearest Neighbors (kNN).\n",
    "\n",
    "  Best for:\n",
    "    - Digit Recognition - kNN achieves 97% on MNIST by comparing pixel similarities\n",
    "    - Recommendation Systems - \"Users who liked X also liked Y\" based on similarity\n",
    "    - Anomaly Detection - Points far from k neighbors are likely anomalies\n",
    "\n",
    "- **Bayesian Models**: Naive Bayes, Gaussian Naive Bayes.\n",
    "\n",
    "  Best for:\n",
    "    - Spam Detection - Naive Bayes perfect for word probabilities in spam/ham emails\n",
    "    - Document Classification - 20 Newsgroups dataset with 90% accuracy\n",
    "    - Real-time Prediction - Extremely fast training and prediction\n",
    "\n",
    "- **Neural Networks**: Feedforward Neural Networks, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs).\n",
    "\n",
    "  Best for:\n",
    "    - ImageNet - CNNs achieve 90%+ accuracy on 1000 classes\n",
    "    - Stock Price Prediction - LSTMs capture temporal patterns in time series\n",
    "    - Natural Language - Transformers (BERT, GPT) revolutionized text understanding\n",
    "\n",
    "- **Ensemble Methods**: Random Forests, Gradient Boosting Machines, Bagging, Boosting.\n",
    "\n",
    "  Best for:\n",
    "    - Any Kaggle Competition - Ensemble of models almost always wins\n",
    "    - Imbalanced Datasets - Balanced Random Forest handles class imbalance\n",
    "    - Production Systems - Combine multiple models for robustness\n",
    "\n",
    "By trying multiple algorithms from different families, you can compare their performance metrics (e.g., accuracy, precision, recall, F1-score, mean squared error) and select the one that performs best on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce9da4-6bfd-4d81-b3b1-36bf89069790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
