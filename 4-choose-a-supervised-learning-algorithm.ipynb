{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "![Choose a Supervised Learning Algorithm banner](./images/4_choose_a_supervised_learning_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "# 4. Choose an algorithm\n",
    "\n",
    "Select an appropriate supervised learning algorithm based on the problem type (classification or regression), data characteristics, interpretability requirements, training time, and other practical considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0dcf1-b0e1-4bf1-b0e8-15d0eb60c60e",
   "metadata": {},
   "source": [
    "## 4.1. Consider algorithm categories\n",
    "\n",
    "![Choose supervised learning algorithm cheat sheet](./images/4_choose_a_supervised_learning_algorithm_scikit_learn_cheat_sheet.png)\n",
    "\n",
    "Machine learning algorithms can be broadly categorized into three main types:\n",
    "\n",
    "**Supervised Learning**: These algorithms learn from labeled data, where the input data has corresponding output labels or target variables. The goal is to learn a mapping function from the input features to the output labels. Common supervised learning tasks include classification (predicting a categorical label) and regression (predicting a continuous value).\n",
    "\n",
    "- **Examples**: Linear Regression, Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVMs), Neural Networks.\n",
    "\n",
    "**Unsupervised Learning**: These algorithms learn from unlabeled data, where there are no predefined output labels. The goal is to discover patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features), and association rule mining.\n",
    "\n",
    "- **Examples**: K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), Association Rule Mining.\n",
    "\n",
    "**Semi-Supervised Learning**: These algorithms combine a small amount of labeled data with a large amount of unlabeled data. They leverage the strengths of both supervised and unsupervised learning techniques to improve model performance, especially when labeled data is scarce or expensive to obtain.\n",
    "\n",
    "- **Examples**: Self-Training, Co-Training, Generative Adversarial Networks (GANs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad863d4-3862-4f52-b611-69c5d0bf3ed9",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 4.2. Evaluate algorithm characteristics\n",
    "\n",
    "When evaluating different supervised learning algorithms, consider the following characteristics:\n",
    "\n",
    "- **Interpretability**: Some algorithms, like linear models and decision trees, are more interpretable and provide insights into the relationship between input features and the target variable. Others, like neural networks, are more complex and can be treated as \"black boxes.\"\n",
    "\n",
    "- **Training Time**: Some algorithms, like linear models, are computationally efficient and can be trained quickly, even on large datasets. Others, like ensemble methods (e.g., Random Forests) and neural networks, may require more computational resources and longer training times.\n",
    "\n",
    "- **Prediction Speed**: After training, some algorithms can make predictions very quickly (e.g., linear models), while others may be slower (e.g., instance-based methods like k-Nearest Neighbors).\n",
    "\n",
    "- **Data Type Handling**: Some algorithms can handle different data types (e.g., categorical, numerical, text) natively, while others may require additional data preprocessing or feature engineering.\n",
    "\n",
    "- **Robustness to Outliers**: Some algorithms, like decision trees and ensemble methods, are more robust to outliers in the data, while others, like linear models, can be heavily influenced by outliers.\n",
    "\n",
    "- **Scalability**: As the size of the dataset grows, some algorithms may become computationally expensive or require specialized techniques (e.g., online learning, distributed computing) to handle large-scale data.\n",
    " \n",
    "Evaluating these characteristics can help narrow down the choices and select algorithms that are well-suited for your specific problem and data characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d767ab5-b6b8-4a61-95c1-0a193d61f736",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 4.3. Try multiple algorithms\n",
    "\n",
    "Since it's difficult to know the best algorithm upfront, it's recommended to try multiple algorithms from different families (e.g., linear models, tree-based models, instance-based models, etc.) and compare their performance on specific data. \n",
    "\n",
    "This process is often referred to as \"model selection\" or \"algorithm selection.\"\n",
    "\n",
    "Here are some common algorithm families and examples:\n",
    "\n",
    "- **Linear Models**: Linear Regression, Logistic Regression, Support Vector Machines (SVMs).\n",
    "\n",
    "- **Tree-Based Models**: Decision Trees, Random Forests, Gradient Boosting Machines.\n",
    "Instance-Based Models: k-Nearest Neighbors (kNN).\n",
    "\n",
    "- **Bayesian Models**: Naive Bayes, Gaussian Naive Bayes.\n",
    "\n",
    "- **Neural Networks**: Feedforward Neural Networks, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs).\n",
    "\n",
    "- **Ensemble Methods**: Random Forests, Gradient Boosting Machines, Bagging, Boosting.\n",
    "\n",
    "By trying multiple algorithms from different families, you can compare their performance metrics (e.g., accuracy, precision, recall, F1-score, mean squared error) and select the one that performs best on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce9da4-6bfd-4d81-b3b1-36bf89069790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
