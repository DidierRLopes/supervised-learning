{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Supervised Learning Steps</summary>\n",
    "    \n",
    "1. Data Collection\n",
    "   * 1.1\\. Data Sources\n",
    "   * 1.2\\. Data Collection Considerations\n",
    "2. Data Exploration and Preparation\n",
    "   * 2.1\\. Data Exploration\n",
    "   * 2.2\\. Data Preparation/Cleaning\n",
    "3. Split Data into Training and Test Sets\n",
    "   * 3.1\\. Holdout Method\n",
    "   * 3.2\\. Cross Validation\n",
    "   * 3.3\\. Data Leakage\n",
    "   * 3.4\\. Best Practices\n",
    "4. Choose a Supervised Learning Algorithm\n",
    "   * 4.1\\. Consider algorithm categories\n",
    "   * 4.2\\. Evaluate algorithm characteristics\n",
    "   * 4.3\\. Try multiple algorithms\n",
    "5. Train the Model\n",
    "   * 5.1\\. Objective Function (Loss/Cost Function)\n",
    "   * 5.2\\. Optimization Algorithms\n",
    "   * 5.3\\. Overfitting and Underfitting\n",
    "6. Evaluate Model Performance\n",
    "   * 6.1\\. Evaluate Model Performance\n",
    "   * 6.2\\. Performance Metrics for Classification Models\n",
    "   * 6.3\\. Interpreting and Reporting Model Performance\n",
    "7. Model Tuning and Selection\n",
    "   * 7.1\\. Hyperparameter Tuning\n",
    "   * 7.2\\. Ensemble Methods\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "# 5. Train the Model\n",
    "\n",
    "![image.png](https://pbs.twimg.com/media/D3SwgeEWAAAaSEv.jpg)\n",
    "\n",
    "## 5.1. Objective Function (Loss/Cost Function)\n",
    "\n",
    "The objective function, also known as the loss or cost function, measures how well the model's predictions match the true labels in the training data. The goal is to minimize this function during training. Common loss functions include:\n",
    "\n",
    "- **For regression**: Mean Squared Error (MSE), Mean Absolute Error (MAE)\n",
    "- **For classification**: Cross-entropy loss, hinge loss\n",
    "\n",
    "The loss is calculated over the entire training dataset and is a key metric to monitor during training.\n",
    "\n",
    "## 5.2. Optimization Algorithms\n",
    "\n",
    "To find the model parameters (weights and biases) that minimize the loss function, optimization algorithms are used. Some popular ones:\n",
    "\n",
    "- **Gradient Descent**: Updates parameters in the direction of the negative gradient of the loss function.\n",
    "- **Stochastic Gradient Descent (SGD)**: Estimates the gradient from a single example or subset of examples instead of the full dataset, allowing faster iterations.\n",
    "- **Adam Optimizer**: An extension of SGD that adapts the learning rate for each parameter, providing faster convergence.\n",
    "\n",
    "## 5.3. Overfitting and Underfitting\n",
    "\n",
    "It's crucial to address the concepts of overfitting and underfitting when training models. Underfitting is caused by high bias (oversimplified model), while overfitting is caused by high variance (overly complex model that captures noise). The goal is to find the right balance between bias and variance by selecting an appropriate model complexity that can capture the true patterns in the data without overfitting.\n",
    "\n",
    "### Overfitting (high variance and low bias)\n",
    "\n",
    "Overfitting occurs when the model learns the training data too well, including the noise, and fails to generalize to new unseen data. This leads to poor performance on the test/validation set despite high training accuracy.\n",
    "\n",
    "Variance refers to the amount that the model's predictions fluctuate when trained on different subsets of the training data. High variance indicates that the model is overly complex and sensitive to noise in the training data.\n",
    "\n",
    "- High variance models are overly complex and can capture noise and random fluctuations in the training data instead of the true signal, leading to overfitting.\n",
    "\n",
    "- Overfitting occurs when a model has high variance and low bias. The model fits the training data too well, including noise, but fails to generalize well to new unseen data, resulting in poor test performance despite high training accuracy.\n",
    "\n",
    "### Underfitting (high bias and low variance)\n",
    "\n",
    "Underfitting happens when the model is too simple and cannot capture the underlying patterns in the data, resulting in poor performance on both training and test sets.\n",
    "\n",
    "Bias refers to the error introduced by overly simplistic assumptions in the learning algorithm. It is the inability of the model to capture the true underlying relationship between the input features and target variable.\n",
    "\n",
    "- High bias models are oversimplified and cannot learn the complex patterns in the data, leading to underfitting.\n",
    "\n",
    "- Underfitting occurs when a model has high bias and low variance. The model is too simple and fails to accurately represent the training data, resulting in poor performance on both training and test sets.\n",
    "\n",
    "\n",
    "### Preventing Overfitting\n",
    "\n",
    "Methods to prevent overfitting:\n",
    "\n",
    "- **Regularization techniques**:\n",
    "    - **L1 (Lasso) Regularization**: Adds the sum of absolute values of weights, driving some weights to zero for sparse models.\n",
    "    - **L2 (Ridge) Regularization**: Adds the sum of squared weights, keeping all weights non-zero but small.\n",
    "- **Dropout**: Randomly drops units from the neural network during training to prevent co-adaptation of features.\n",
    "- **Early Stopping**: Stop training when validation error starts increasing\n",
    "- **Cross-validation**: Splitting the data into training, validation, and test sets. The validation set is used to tune hyperparameters and monitor for overfitting during training.\n",
    "- **Data augmentation**: Increasing the size and diversity of the training data by applying transformations like flipping, rotating, or adding noise. This helps the model generalize better.\n",
    "- **Reducing model complexity**: Using a simpler model with fewer parameters, or techniques like pruning to remove unnecessary connections.\n",
    "- **Ensemble methods**: Combining multiple models, such as bagging or boosting, to reduce variance and overfitting\n",
    "\n",
    "### Preventing Underfitting\n",
    "\n",
    "Methods to prevent underfitting:\n",
    "\n",
    "- **Increasing model complexity**: Using a more complex model with more parameters or layers to capture the underlying patterns in the data.\n",
    "- **Feature engineering**: Adding more relevant features or transforming existing ones to better represent the data.\n",
    "- **Removing noise**: Cleaning and preprocessing the data to remove irrelevant or noisy features.\n",
    "- **Increasing training time**: Training the model for more epochs or iterations to allow it to learn the patterns better.\n",
    "- **Reducing regularization**: Decreasing the regularization strength if it is causing underfitting by overly constraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d00be-d272-4bbe-90c6-2349b1658ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
