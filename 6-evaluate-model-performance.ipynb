{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Supervised Learning Steps</summary>\n",
    "    \n",
    "1. Data Collection\n",
    "   * 1.1\\. Data Sources\n",
    "   * 1.2\\. Data Collection Considerations\n",
    "2. Data Exploration and Preparation\n",
    "   * 2.1\\. Data Exploration\n",
    "   * 2.2\\. Data Preparation/Cleaning\n",
    "3. Split Data into Training and Test Sets\n",
    "   * 3.1\\. Holdout Method\n",
    "   * 3.2\\. Cross Validation\n",
    "   * 3.3\\. Data Leakage\n",
    "   * 3.4\\. Best Practices\n",
    "4. Choose a Supervised Learning Algorithm\n",
    "   * 4.1\\. Consider algorithm categories\n",
    "   * 4.2\\. Evaluate algorithm characteristics\n",
    "   * 4.3\\. Try multiple algorithms\n",
    "5. Train the Model\n",
    "   * 5.1\\. Objective Function (Loss/Cost Function)\n",
    "   * 5.2\\. Optimization Algorithms\n",
    "   * 5.3\\. Overfitting and Underfitting\n",
    "6. Evaluate Model Performance\n",
    "   * 6.1\\. Evaluate Model Performance\n",
    "   * 6.2\\. Performance Metrics for Classification Models\n",
    "   * 6.3\\. Interpreting and Reporting Model Performance\n",
    "7. Model Tuning and Selection\n",
    "   * 7.1\\. Hyperparameter Tuning\n",
    "   * 7.2\\. Ensemble Methods\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "### 6.1. Performance Metrics for Regression Models\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "- Calculates the average squared difference between predicted and actual values\n",
    "- Squaring the errors gives more weight to larger errors\n",
    "- Sensitive to outliers, as squaring amplifies the effect of large errors\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**\n",
    "- Square root of MSE, providing the same units as the target variable\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "- Calculates the average absolute difference between predicted and actual values\n",
    "- Less sensitive to outliers compared to MSE/RMSE\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "**R-squared (Coefficient of Determination)**\n",
    "- Measures the proportion of variance in the target variable that is explained by the model\n",
    "- Ranges from 0 to 1, with 1 indicating a perfect fit\n",
    "- Useful for comparing different models, but can be misleading in some cases\n",
    "\n",
    "**Residual Analysis**\n",
    "- Residuals: Differences between predicted and actual values\n",
    "- Residual plots: Visualize residuals against predicted values or other features\n",
    "- Identify patterns, outliers, and violations of regression assumptions\n",
    "- Useful for diagnosing issues with the model or data\n",
    " \n",
    "### 6.2. Performance Metrics for Classification Models\n",
    "\n",
    "**Accuracy**\n",
    "- Definition: Proportion of correct predictions out of total predictions\n",
    "- Pros: Simple and easy to understand\n",
    "- Cons: Can be misleading for imbalanced datasets, doesn't provide insight into types of errors\n",
    "\n",
    "**Precision, Recall, and F1-score**\n",
    "- Precision: Proportion of true positives out of predicted positives (how many positives were actually correct)\n",
    "- Recall: Proportion of true positives out of actual positives (how many actual positives were correctly identified)\n",
    "- F1-score: Harmonic mean of precision and recall, provides a balanced measure\n",
    "Confusion Matrix\n",
    "\n",
    "**Tabular representation of correct and incorrect predictions**\n",
    "- Rows represent actual classes, columns represent predicted classes\n",
    "- Provides insights into types of errors (false positives, false negatives)\n",
    "\n",
    "**Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)**\n",
    "- ROC curve: Plots true positive rate (recall) against false positive rate at different classification thresholds\n",
    "- AUC: Area under the ROC curve, ranges from 0 to 1 (higher is better)\n",
    "- Useful for evaluating binary classification models and comparing different models\n",
    "- AUC of 0.5 indicates a random classifier, while 1.0 indicates a perfect classifier\n",
    "\n",
    "**Precision-Recall Curve**\n",
    "- Plots precision against recall at different classification thresholds\n",
    "- Useful for imbalanced datasets and applications where recall is more important than precision (or vice versa)\n",
    "- Precision-recall curves are more informative than ROC curves for imbalanced datasets\n",
    "\n",
    "**Metrics for Multi-class Classification**\n",
    "- Micro-averaging: Calculates metrics globally by treating all classes as one\n",
    "- Macro-averaging: Calculates metrics for each class and takes the average\n",
    "- Weighted averaging: Calculates metrics for each class and takes the weighted average based on class frequencies\n",
    "\n",
    "### 6.3. Interpreting and Reporting Model Performance\n",
    "\n",
    "**Analyzing and Interpreting Evaluation Metrics**\n",
    "- Understand the strengths and weaknesses of different metrics\n",
    "- Consider the problem context and business requirements\n",
    "- Analyze trade-offs between different metrics (e.g., precision vs. recall)\n",
    "\n",
    "**Visualizing Model Performance**\n",
    "- Learning curves: Plot performance metric against training set size to diagnose bias and variance issues\n",
    "- Residual plots: Plot residuals (actual - predicted) against predicted values or other features to identify patterns and outliers\n",
    "\n",
    "**Comparing Model Performance**\n",
    "- Use appropriate statistical tests (e.g., paired t-test, McNemar's test) to determine if performance differences are statistically significant\n",
    "- Consider confidence intervals and effect sizes in addition to point estimates\n",
    "\n",
    "**Reporting Model Performance**\n",
    "- Report performance metrics on a held-out test set (not the training or validation set)\n",
    "- Include confidence intervals or standard deviations for performance estimates\n",
    "- Describe the evaluation methodology (data splitting, cross-validation, etc.)\n",
    "- Discuss limitations, assumptions, and potential sources of bias or error\n",
    "Follow best practices and guidelines for reporting machine learning results (e.g., MLCommons, NIST guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d00be-d272-4bbe-90c6-2349b1658ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
