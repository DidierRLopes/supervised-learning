{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Supervised Learning Steps</summary>\n",
    "    \n",
    "1. Data Collection\n",
    "   * 1.1\\. Data Sources\n",
    "   * 1.2\\. Data Collection Considerations\n",
    "2. Data Exploration and Preparation\n",
    "   * 2.1\\. Data Exploration\n",
    "   * 2.2\\. Data Preparation/Cleaning\n",
    "3. Split Data into Training and Test Sets\n",
    "   * 3.1\\. Holdout Method\n",
    "   * 3.2\\. Cross Validation\n",
    "   * 3.3\\. Data Leakage\n",
    "   * 3.4\\. Best Practices\n",
    "4. Choose a Supervised Learning Algorithm\n",
    "   * 4.1\\. Consider algorithm categories\n",
    "   * 4.2\\. Evaluate algorithm characteristics\n",
    "   * 4.3\\. Try multiple algorithms\n",
    "5. Train the Model\n",
    "   * 5.1\\. Objective Function (Loss/Cost Function)\n",
    "   * 5.2\\. Optimization Algorithms\n",
    "   * 5.3\\. Overfitting and Underfitting\n",
    "6. Evaluate Model Performance\n",
    "   * 6.1\\. Performance Metrics for Regression Models\n",
    "   * 6.2\\. Performance Metrics for Classification Models\n",
    "7. Model Tuning and Selection\n",
    "   * 7.1\\. Hyperparameter Tuning\n",
    "   * 7.2\\. Ensemble Methods\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1280/0*6syXK-mCaQnvmjgS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcd29a-af6d-4648-a783-d0eff85fdbf7",
   "metadata": {},
   "source": [
    "### 6.1. Performance Metrics for Regression Models\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2\n",
    "\\end{equation*}\n",
    "\n",
    "- Calculates the average squared difference between predicted and actual values\n",
    "- Squaring the errors gives more weight to larger errors\n",
    "- Sensitive to outliers, as squaring amplifies the effect of large errors\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- Square root of MSE, providing the same units as the target variable\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{n} \\Sigma_{i=1}^n |{y}-\\hat{y}|\n",
    "\\end{equation*}\n",
    "\n",
    "- Calculates the average absolute difference between predicted and actual values\n",
    "- Less sensitive to outliers compared to MSE/RMSE\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "#### R-squared (Coefficient of Determination)\n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- Measures the proportion of variance in the target variable that is explained by the model\n",
    "- Ranges from 0 to 1, with 1 indicating a perfect fit\n",
    "- Useful for comparing different models, but can be misleading in some cases\n",
    "\n",
    "#### Residual Analysis\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i - \\hat{y}_i\n",
    "\\end{equation*}\n",
    "\n",
    "- Residuals: Differences between predicted and actual values\n",
    "- Residual plots: Visualize residuals against predicted values or other features\n",
    "- Identify patterns, outliers, and violations of regression assumptions\n",
    "- Useful for diagnosing issues with the model or data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf9358-4332-4e84-ab74-cf3e00f2ff14",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### 6.2. Performance Metrics for Classification Models\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "![image](https://miro.medium.com/v2/0*-oGC3SE8sPCPdmxs.jpg)\n",
    "\n",
    "(Sensitivity = recall)\n",
    "\n",
    "A few notes:\n",
    "- Accuracy is simple and easy to understand. It can be misleading for imbalanced datasets, as it doesn't provide insight into types of errors\n",
    "- Precision is the proportion of true positives out of predicted positives (how many positives were actually correct)\n",
    "- Recall is the roportion of true positives out of actual positives (how many actual positives were correctly identified)\n",
    "\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "The F1 score is a measure that combines precision and recall into a single metric for evaluating the performance of a classification model. It is calculated as the harmonic mean of precision and recall.\n",
    "\n",
    "The formula for the F1 score is:\n",
    "\n",
    "\\begin{equation*}\n",
    "F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation*}\n",
    "\n",
    "The F1 score ranges from 0 to 1, with 1 being the best possible value, indicating perfect precision and recall\n",
    "\n",
    "- It provides a balanced way to combine precision and recall into a single metric.\n",
    "- It is particularly useful when there is an uneven class distribution (imbalanced dataset) since accuracy alone can be misleading in such cases.\n",
    "- A high F1 score indicates that the model has both high precision (minimizing false positives) and high recall (minimizing false negatives).\n",
    "- It is widely used in areas like information retrieval, natural language processing, and machine learning classification tasks.\n",
    "- The F1 score can be adjusted to give more weight to precision (F0.5 score) or recall (F2 score) based on the specific requirements of the problem.\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "![image](https://i0.wp.com/sefiks.com/wp-content/uploads/2020/12/roc-curve-original.png?ssl=1)\n",
    "\n",
    "The ROC curve and AUC are useful for evaluating and comparing the performance of binary classification models.\n",
    "\n",
    "**ROC Curve**\n",
    "\n",
    "The ROC curve is a graphical representation of the performance of a binary classification model at different classification thresholds. It plots the True Positive Rate (TPR) or Recall on the y-axis against the False Positive Rate (FPR) on the x-axis.\n",
    "- True Positive Rate (TPR) or Recall = $\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$\n",
    "- False Positive Rate (FPR) =  $\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$\n",
    "\n",
    "The ROC curve is created by varying the classification threshold from 0 to 1 and calculating the TPR and FPR at each threshold. The curve shows the trade-off between the TPR and FPR for different thresholds.\n",
    "\n",
    "The ROC curve provides a visual representation of the trade-off between the TPR and FPR, allowing you to choose an appropriate classification threshold based on the desired balance between these two metrics.\n",
    "\n",
    "**AUC**\n",
    "\n",
    "The Area Under the ROC Curve (AUC) is a single scalar value that summarizes the overall performance of the binary classifier across all possible classification thresholds. It ranges from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "- An AUC of 1.0 represents a perfect classifier that can correctly classify all instances.\n",
    "- An AUC of 0.5 represents a random classifier, where the model's predictions are no better than a random guess.\n",
    "- An AUC of 0.0 represents a classifier that is completely wrong in its predictions.\n",
    "\n",
    "The AUC can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
    "\n",
    "The AUC is a single metric that summarizes the overall performance of the model, making it easier to compare different models or different configurations of the same model.\n",
    "\n",
    "#### Precision-Recall Curve\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/0*1z69voTBb04MIzig)\n",
    "\n",
    "A Precision-Recall Curve is a plot that visualizes the trade-off between precision and recall for different probability thresholds in a binary classification model. The curve shows the precision (y-axis) against the recall (x-axis) at various thresholds.\n",
    "\n",
    "A high precision indicates a low false positive rate, while a high recall indicates a low false negative rate. The ideal model would have both precision and recall equal to 1, but typically there is a trade-off where increasing precision reduces recall and vice versa.\n",
    "\n",
    "So this curve is useful for imbalanced datasets and applications where recall is more important than precision (or vice versa), as it helps visualize this trade-off and select an appropriate threshold based on the desired balance of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1086d-2ba4-436a-bb4a-db2280034bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
