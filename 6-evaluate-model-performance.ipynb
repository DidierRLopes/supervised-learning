{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab9ec67-891b-4ecc-92ab-34f07aa505cd",
   "metadata": {},
   "source": [
    "![Evaluate Model Performance banner](./images/6_evaluate_model_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886a88-569a-4ccb-a254-e5aa29eac62e",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcd29a-af6d-4648-a783-d0eff85fdbf7",
   "metadata": {},
   "source": [
    "### 6.1. Performance Metrics for Regression Models\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2\n",
    "\\end{equation*}\n",
    "\n",
    "- Calculates the average squared difference between predicted and actual values\n",
    "- Squaring the errors gives more weight to larger errors\n",
    "- Sensitive to outliers, as squaring amplifies the effect of large errors\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- Square root of MSE, providing the same units as the target variable\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{n} \\Sigma_{i=1}^n |{y}-\\hat{y}|\n",
    "\\end{equation*}\n",
    "\n",
    "- Calculates the average absolute difference between predicted and actual values\n",
    "- Less sensitive to outliers compared to MSE/RMSE\n",
    "- Easier to interpret than MSE, as it represents the typical magnitude of error\n",
    "\n",
    "#### R-squared (Coefficient of Determination)\n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- Measures the proportion of variance in the target variable that is explained by the model\n",
    "- Ranges from 0 to 1, with 1 indicating a perfect fit\n",
    "- Useful for comparing different models, but can be misleading in some cases\n",
    "\n",
    "#### Residual Analysis\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i - \\hat{y}_i\n",
    "\\end{equation*}\n",
    "\n",
    "- Residuals: Differences between predicted and actual values\n",
    "- Residual plots: Visualize residuals against predicted values or other features\n",
    "- Identify patterns, outliers, and violations of regression assumptions\n",
    "- Useful for diagnosing issues with the model or data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf9358-4332-4e84-ab74-cf3e00f2ff14",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### 6.2. Performance Metrics for Classification Models\n",
    "\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "![Evaluate Model Performance banner](./images/6_evaluate_model_performance_confusion_matrix.png)\n",
    "\n",
    "\n",
    "### Types of Error\n",
    "\n",
    "Type I and Type II errors are fundamental concepts in statistics and machine learning related to hypothesis testing. They refer to the two types of errors that can occur when making decisions based on statistical analysis.\n",
    "\n",
    "**Type I Error (False Positive)**:\n",
    "- A Type I error, also known as a false positive, occurs when the null hypothesis is true, but it is incorrectly rejected. In other words, it is the error of detecting an effect or pattern that does not actually exist.\n",
    "- In machine learning, a Type I error could occur when a model incorrectly classifies a negative instance as positive. For example, in a spam email detection system, a Type I error would be classifying a legitimate email as spam.\n",
    "\n",
    "**Type II Error (False Negative)**:\n",
    "- A Type II error, also known as a false negative, occurs when the null hypothesis is false, but it is incorrectly accepted. In other words, it is the error of failing to detect an effect or pattern that does exist.\n",
    "- In machine learning, a Type II error could occur when a model incorrectly classifies a positive instance as negative. For example, in a disease diagnosis system, a Type II error would be classifying a diseased patient as healthy.\n",
    "\n",
    "The trade-off between Type I and Type II errors is controlled by adjusting the decision threshold or confidence level in statistical tests or machine learning models. Reducing the probability of one type of error generally increases the probability of the other type of error.\n",
    "\n",
    "The choice of which error is more tolerable depends on the specific problem and the consequences associated with each type of error.\n",
    "\n",
    "### Accuracy, Precision and Recall\n",
    "\n",
    "- **Accuracy** is simple and easy to understand. It can be misleading for imbalanced datasets, as it doesn't provide insight into types of errors\n",
    "- **Precision** is the proportion of true positives out of predicted positives (how many positives were actually correct)\n",
    "- **Recall** is the roportion of true positives out of actual positives (how many actual positives were correctly identified)\n",
    "\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "The F1 score is a measure that combines precision and recall into a single metric for evaluating the performance of a classification model. It is calculated as the harmonic mean of precision and recall.\n",
    "\n",
    "The formula for the F1 score is:\n",
    "\n",
    "\\begin{equation*}\n",
    "F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation*}\n",
    "\n",
    "The F1 score ranges from 0 to 1, with 1 being the best possible value, indicating perfect precision and recall\n",
    "\n",
    "- It provides a balanced way to combine precision and recall into a single metric.\n",
    "- It is particularly useful when there is an uneven class distribution (imbalanced dataset) since accuracy alone can be misleading in such cases.\n",
    "- A high F1 score indicates that the model has both high precision (minimizing false positives) and high recall (minimizing false negatives).\n",
    "- It is widely used in areas like information retrieval, natural language processing, and machine learning classification tasks.\n",
    "- The F1 score can be adjusted to give more weight to precision (F0.5 score) or recall (F2 score) based on the specific requirements of the problem.\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "![Evaluate Model Performance ROC Curve](./images/6_evaluate_model_performance_roc_curve.png)\n",
    "\n",
    "The ROC curve and AUC are useful for evaluating and comparing the performance of binary classification models.\n",
    "\n",
    "**ROC Curve**\n",
    "\n",
    "The ROC curve is a graphical representation of the performance of a binary classification model at different classification thresholds. It plots the True Positive Rate (TPR) or Recall on the y-axis against the False Positive Rate (FPR) on the x-axis.\n",
    "- True Positive Rate (TPR) or Recall = $\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$\n",
    "- False Positive Rate (FPR) =  $\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$\n",
    "\n",
    "The ROC curve is created by varying the classification threshold from 0 to 1 and calculating the TPR and FPR at each threshold. The curve shows the trade-off between the TPR and FPR for different thresholds.\n",
    "\n",
    "The ROC curve provides a visual representation of the trade-off between the TPR and FPR, allowing you to choose an appropriate classification threshold based on the desired balance between these two metrics.\n",
    "\n",
    "**AUC**\n",
    "\n",
    "The Area Under the ROC Curve (AUC) is a single scalar value that summarizes the overall performance of the binary classifier across all possible classification thresholds. It ranges from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "- An AUC of 1.0 represents a perfect classifier that can correctly classify all instances.\n",
    "- An AUC of 0.5 represents a random classifier, where the model's predictions are no better than a random guess.\n",
    "- An AUC of 0.0 represents a classifier that is completely wrong in its predictions.\n",
    "\n",
    "The AUC can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
    "\n",
    "The AUC is a single metric that summarizes the overall performance of the model, making it easier to compare different models or different configurations of the same model.\n",
    "\n",
    "#### Precision-Recall Curve\n",
    "\n",
    "![Evaluate Model Performance PR Curve](./images/6_evaluate_model_performance_pr_curve.png)\n",
    "\n",
    "A Precision-Recall Curve is a plot that visualizes the trade-off between precision and recall for different probability thresholds in a binary classification model. The curve shows the precision (y-axis) against the recall (x-axis) at various thresholds.\n",
    "\n",
    "A high precision indicates a low false positive rate, while a high recall indicates a low false negative rate. The ideal model would have both precision and recall equal to 1, but typically there is a trade-off where increasing precision reduces recall and vice versa.\n",
    "\n",
    "So this curve is useful for imbalanced datasets and applications where recall is more important than precision (or vice versa), as it helps visualize this trade-off and select an appropriate threshold based on the desired balance of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1086d-2ba4-436a-bb4a-db2280034bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
